
// example of a result file for training
{
    "config": {
        "GNN_model": "GCN(1433, 64, 4)",
        "implementation": "Example-memory",
        "dataset": "ogbn-exapmle",
        "num_epochs": 5,
        "batch_size": 1024,
        "learning_rate": 0.01,
        "weight_decay": 5e-4,
        "num_workers_training": 4,
        "nbr_GPUs_used": 0
    },

    "results": {
        "success": true, // if true all fields below are set

        "total_training_time": 120.5,
        "batch_times": [0.5, 0.4, 0.3, 0.2, 0.1], // delete this one
        "nbr_batches": 100,

        "train_loss": [0.5, 0.4, 0.3, 0.2, 0.1],
        "validation_loss": [0.45, 0.35, 0.25, 0.15, 0.05],
        
        "test_accuracy": 0.2,
        "test_inference_time": 10.0,
        "validation_accuracy": [{"start":0.5, "end":0.6, "value":0.55}, {"start":0.5, "end":0.6, "value":0.55}],

        // these three together lets us calculate the average sampling and feature retrieval time
        "total_sampling_time": 1000.0,
        "total_feature_retrieval_time": 10000.0,
        "total_sampling_calls": 100,

        // this lets us compare where the program is spending its time during training (sampling vs feature retrieval vs training)
        "model_training_time": 500.0,
        "total_training_iterations": 100,

        // cache performance metrics
        "Sampled_nodes_total": 1000,
        "Sampled_nodes_cache": 300,


        "GPU_utilization": [0.5, 0.6, 0.7, 0.8, 0.9],
        "GPU_memory_usage": [1000, 1500, 2000, 2500, 3000],
        "CPU_utilization": [0.5, 0.6, 0.7, 0.8, 0.9],
        "RAM_usage": [4, 5, 6, 7, 8]
    }
}